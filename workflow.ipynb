{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an Antarctic Rift Catalog\n",
    "\n",
    "## Step 0: Decide which data to use\n",
    "This is done using IcePyx to spatially window available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelf_name = 'brunt'\n",
    "\n",
    "# Format is WSNE\n",
    "# spatial_extent = [-26.5, -75.8, -24.5, -75.3] # Brunt\n",
    "# spatial_extent = [-27.8, -76.1, -23.1, -75.2] # Bigger Brunt\n",
    "spatial_extent = [-27.8, -76.1, -3.0, -70.2] # brunt-riiser-ekstrom\n",
    "# spatial_extent = [-3.0,-71.5, 39.5, -68.6] # Fimbul\n",
    "# spatial_extent = [67.6, -72.44,74.87,-68.39] # Amery\n",
    "# spatial_extent = [159, -86, 180, -69] # East Ross\n",
    "# spatial_extent = [-180, -86, -140, -77] # West Ross\n",
    "# spatial_extent = [-65.5,-68.7,-60.2,-66] # Larsen C\n",
    "# spatial_extent = [-82.0, 82.4, -79.5, 82.9] # Milne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filelist written with 1742 files in it.\n"
     ]
    }
   ],
   "source": [
    "# If filelist_file_name exists, then load it.  Otherwise query icepyx\n",
    "\n",
    "# file_path = 's3://its-live-data.jpl.nasa.gov/icesat2/alt06/rel003/'\n",
    "file_path = '/Users/lipovsky/is2-data/' + shelf_name + '/'\n",
    "\n",
    "import os.path\n",
    "import icepyx as ipx\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "filelist_file_name = file_path + shelf_name + '_filelist.pickle'\n",
    "if os.path.isfile(filelist_file_name):\n",
    "    with open(filelist_file_name, 'rb') as handle:\n",
    "        file_list = pickle.load(handle)\n",
    "else:       \n",
    "    date_range = ['2018-10-14','2020-12-01']\n",
    "    region_a = ipx.Query('ATL06', spatial_extent, date_range)\n",
    "    granules=region_a.granules.avail\n",
    "\n",
    "    file_list = []\n",
    "    for f in granules: \n",
    "        file_list.append(file_path + f['producer_granule_id'])\n",
    "        \n",
    "    with open(filelist_file_name, 'wb') as handle:\n",
    "        pickle.dump(file_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('Filelist written with %d files in it.'%len(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read the ATL06 files into a Python data structure\n",
    "Put the needed info in a dictionary, save the whole thing to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lipovsky/is2-data/brunt/ATL06_20181014041230_02370110_003_01.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/Users/lipovsky/is2-data/brunt/ATL06_20181014041230_02370110_003_01.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b3a191301b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0matl06_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshelf_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_atl06.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmaskfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/lipovsky/Downloads/BedMachineAntarctica_2020-07-15_v02.nc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0marc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matl06_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaskfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Science/brunt/arc.py\u001b[0m in \u001b[0;36mingest\u001b[0;34m(file_list, output_file_name, maskfile)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"l\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                swmr=swmr)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/Users/lipovsky/is2-data/brunt/ATL06_20181014041230_02370110_003_01.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import arc\n",
    "import numpy as np\n",
    "import importlib\n",
    "importlib.reload(arc)\n",
    "\n",
    "\n",
    "atl06_file_name = shelf_name + '_atl06.pickle'\n",
    "maskfile = '/Users/lipovsky/Downloads/BedMachineAntarctica_2020-07-15_v02.nc'\n",
    "arc.ingest(file_list,atl06_file_name,maskfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.  Run the rift detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open(atl06_file_name, 'rb') as handle:\n",
    "    atl06_data = pickle.load(handle)\n",
    "\n",
    "for i in range(len(atl06_data['quality'])):\n",
    "    atl06_data['quality'][i] = np.array(atl06_data['quality'][i])\n",
    "    atl06_data['h'][i] = np.array(atl06_data['h'][i])\n",
    "    atl06_data['geoid'][i] = np.array(atl06_data['geoid'][i])\n",
    "    atl06_data['azimuth'][i] = np.array(atl06_data['azimuth'][i])\n",
    "    atl06_data['h_sig'][i] = np.array(atl06_data['h_sig'][i])\n",
    "\n",
    "\n",
    "# Find the rifts\n",
    "rift_obs = arc.get_rifts(atl06_data)\n",
    "\n",
    "# Store the rifts in a dataframe\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "rift_obs=pd.DataFrame(rift_obs)\n",
    "rift_obs = geopandas.GeoDataFrame(rift_obs,\n",
    "                             geometry=geopandas.points_from_xy(rift_obs['x-centroid'],\n",
    "                                                               rift_obs['y-centroid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the whole rift catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "# image_file = '/Users/lipovsky/Downloads/riise_20_1045_modis_ch02.tif'\n",
    "image_file = '/Users/lipovsky/Downloads/riise_2020028_1615_modis_ch02.tif'\n",
    "\n",
    "# Plot catalog over imagery\n",
    "fig2,ax2 = plt.subplots(figsize=(20,20))\n",
    "sat_data = rasterio.open(image_file)\n",
    "show(sat_data,cmap=\"gray\",ax=ax2)\n",
    "ax2.scatter(rift_obs['x-centroid'] ,rift_obs['y-centroid'] ,s=2,c='r')\n",
    "window_x = [-7.3e5,-6.4e5]\n",
    "window_y = [1.35e6,1.46e6]\n",
    "plt.ylim(window_y)\n",
    "plt.xlim(window_x)\n",
    "\n",
    "# plt.ylim([1.424e6,1.453e6])\n",
    "# plt.xlim([-7.05e5,-6.60e5])\n",
    "\n",
    "# Make a bounding box and only look at the points within the box\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "coords = [(-701000,1424000), (-685000,1427500), (-670000,1435000), (-660000,1450000),\n",
    "         (-655000,1458000),(-685000,1436000),(-703000,1425000)]\n",
    "poly = Polygon(coords)\n",
    "box_x,box_y = poly.exterior.xy\n",
    "plt.plot(box_x,box_y)\n",
    "plt.plot(box_x,box_y)\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ITS_LIVE velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset    \n",
    "d = Dataset('/Users/lipovsky/Downloads/ANT_G0120_0000.nc', 'r')\n",
    "print(d.variables['vx'])\n",
    "itslive_x =d.variables['x']\n",
    "itslive_x = np.array(itslive_x)\n",
    "\n",
    "itslive_y =d.variables['y']\n",
    "itslive_y = np.array(itslive_y)\n",
    "\n",
    "itslive_vx=d.variables['vx']\n",
    "itslive_vy=d.variables['vy']\n",
    "itslive_ocean=d.variables['ocean']\n",
    "\n",
    "x_slice = itslive_x[ (itslive_x<max(window_x)) & (itslive_x>min(window_x)) ]\n",
    "y_slice = itslive_y[ (itslive_y<max(window_y)) & (itslive_y>min(window_y)) ]\n",
    "\n",
    "\n",
    "vx_slice=itslive_vx[ (itslive_y<max(window_y)) & (itslive_y>min(window_y)) , \n",
    "            (itslive_x<max(window_x)) & (itslive_x>min(window_x))]\n",
    "vy_slice=itslive_vy[ (itslive_y<max(window_y)) & (itslive_y>min(window_y)) , \n",
    "            (itslive_x<max(window_x)) & (itslive_x>min(window_x))]\n",
    "ocean_slice=itslive_ocean[ (itslive_y<max(window_y)) & (itslive_y>min(window_y)) , \n",
    "            (itslive_x<max(window_x)) & (itslive_x>min(window_x))]\n",
    "d.close()\n",
    "\n",
    "# Interpolate.  This will be useful later\n",
    "\n",
    "# xmesh[0:3,0:3]\n",
    "\n",
    "from scipy.interpolate import interp2d\n",
    "vx_interp = interp2d(x_slice, y_slice, vx_slice)\n",
    "vy_interp = interp2d(x_slice, y_slice, vy_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique RGTs:  array([ 154,  215,  276,  283,  344,  596,  657,  718,  725,  786, 1099,\n",
    "#       1160, 1167, 1228, 1289\n",
    "# rgt = 596 # Has two nice beam pairs\n",
    "rgt = 657\n",
    "ho = rift_obs[rift_obs.within(poly)]\n",
    "fig2,ax2 = plt.subplots(figsize=(10,10))\n",
    "ax2.scatter(ho['x-centroid'][ho['rgt']==rgt] ,ho['y-centroid'][ho['rgt']==rgt],\n",
    "            s=ho['width'][ho['rgt']==rgt],c=ho['time'][ho['rgt']==rgt],alpha=0.5)\n",
    "skip = 5\n",
    "c = plt.quiver(xmesh[ ::skip, ::skip],\n",
    "               ymesh[ ::skip, ::skip],\n",
    "               vx_slice[ ::skip, ::skip], \n",
    "               vy_slice[ ::skip, ::skip], scale=20000, color='r')\n",
    "\n",
    "plt.xlim((min(ho['x-centroid'][ho['rgt']==rgt])-100, max(ho['x-centroid'][ho['rgt']==rgt])+100))\n",
    "plt.ylim((min(ho['y-centroid'][ho['rgt']==rgt])-100, max(ho['y-centroid'][ho['rgt']==rgt])+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "fig3,ax3=plt.subplots(figsize=(20,20))\n",
    "xmesh,ymesh = np.meshgrid(x_slice,y_slice)\n",
    "\n",
    "skip=75\n",
    "plt.scatter(ho['x-centroid'] ,ho['y-centroid'] ,s=50,c='r',alpha=0.5)\n",
    "plt.contourf(xmesh,ymesh,ocean_slice,cmap=cm.get_cmap('cool',30))\n",
    "show(sat_data,cmap=\"gray\",ax=ax3)\n",
    "c = plt.quiver(xmesh[ ::skip, ::skip],\n",
    "               ymesh[ ::skip, ::skip],\n",
    "               vx_slice[ ::skip, ::skip], \n",
    "               vy_slice[ ::skip, ::skip], scale=9000, color='w')\n",
    "plt.axis('image')\n",
    "plt.ylim(window_y)\n",
    "plt.xlim(window_x)\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Distance (km)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "dvx = np.gradient(vx_slice,y_slice,x_slice)\n",
    "dvy = np.gradient(vx_slice,y_slice,x_slice)\n",
    "\n",
    "# With this convention:\n",
    "# dvx[0] is dvx/dy\n",
    "# dvx[1] is dvx/dx\n",
    "# dvy[0] is dvy/dy\n",
    "# dvy[1] is dvy/dx\n",
    "\n",
    "# Interpolate\n",
    "vx_x = interp2d(x_slice, y_slice, dvx[1])\n",
    "vx_y = interp2d(x_slice, y_slice, dvx[0])\n",
    "vy_x = interp2d(x_slice, y_slice, dvy[1])\n",
    "vy_y = interp2d(x_slice, y_slice, dvy[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the catalog\n",
    "Plot the rift detections along with the ATL06 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "atl06_data=pd.DataFrame(atl06_data)\n",
    "\n",
    "\n",
    "indices = np.unique(rift_obs[\"data_row\"])\n",
    "n=len(indices)\n",
    "fig,ax=plt.subplots(figsize=(20,5))\n",
    "\n",
    "for i in range(1,n):\n",
    "    index = indices[i]\n",
    "    if atl06_data.iloc[index][\"rgt\"] != 657:\n",
    "        continue\n",
    "\n",
    "#     d = np.sqrt(atl06_data.iloc[index][\"x\"]**2 + atl06_data.iloc[index][\"y\"]**2)\n",
    "    d = atl06_data.iloc[index][\"x_atc\"]\n",
    "    h = atl06_data.iloc[index][\"h\"] - atl06_data.iloc[index][\"geoid\"]\n",
    "\n",
    "    # plt.subplot(n,1,i)\n",
    "    plt.scatter(d,h,c=atl06_data.iloc[index][\"quality\"])\n",
    "    rift_list = arc.find_the_rifts(atl06_data.iloc[index][\"h\"])\n",
    "    output = arc.convert_to_centroid(rift_list,atl06_data.iloc[index][\"x\"],atl06_data.iloc[index][\"y\"])\n",
    "\n",
    "\n",
    "    for s in rift_list:\n",
    "        plt.plot(d[s[0]:s[1]],h[s[0]:s[1]],'-or')\n",
    "    plt.title('RGT: %i, Beam: %s, Time: %s'%(atl06_data.iloc[index][\"rgt\"],\n",
    "                                   atl06_data.iloc[index][\"beam\"],\n",
    "                                   atl06_data.iloc[index][\"time\"]))\n",
    "    plt.xlabel('X_atc')\n",
    "        \n",
    "#     xc = np.array(output['x-centroid'])\n",
    "#     yc = np.array(output['y-centroid'])\n",
    "#     dc = np.sqrt(xc**2 + yc**2)\n",
    "#     for dd,ww in zip(dc,output['width']):\n",
    "#         xcoords = (dd-ww/2,dd+ww/2)-d[0]\n",
    "#         plt.plot( xcoords, (0,0),'-ob')\n",
    "#     #     plt.xlim((0,2e3))\n",
    "\n",
    "    plt.ylim((-20,100))\n",
    "    plt.grid()\n",
    "    fig.savefig('figures_brunt/plot%i.png'%i)    \n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify that we loaded the data correctly\n",
    "by making a plot of the ATL06 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "\n",
    "ttstart = datetime.now()\n",
    "fig = plt.figure(figsize=[12, 12])\n",
    "# ax  = plt.subplot(projection=ccrs.SouthPolarStereo())\n",
    "# ax  = plt.subplot(projection=ccrs.NorthPolarStereo(central_longitude=-81.0))\n",
    "ax  = plt.subplot(projection=ccrs.PlateCarree())\n",
    "\n",
    "# ax.coastlines(resolution='50m')\n",
    "# ax.add_feature(cartopy.feature.LAND)\n",
    "\n",
    "ax.gridlines(draw_labels=True, dms=True ,  x_inline=False, y_inline=False)\n",
    "\n",
    "thr = 4\n",
    "sig_thr = 1\n",
    "\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "lon_formatter = LongitudeFormatter(number_format='.1f',\n",
    "                                   degree_symbol='',\n",
    "                                   dateline_direction_label=True)\n",
    "lat_formatter = LatitudeFormatter(number_format='.1f',\n",
    "                                  degree_symbol='')\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "\n",
    "\n",
    "\n",
    "for lat, lon, h, q, r in zip(atl06_data['lat'], \n",
    "                            atl06_data['lon'], \n",
    "                            atl06_data['h'], \n",
    "                            atl06_data['quality'],\n",
    "                            atl06_data['rgt']):\n",
    "    sc = ax.scatter(lon[q==0],lat[q==0],\n",
    "                    c=h[q==0],s=1,vmin=15,vmax=30,alpha=0.5,\n",
    "                    transform=ccrs.PlateCarree())\n",
    "    if r==95:\n",
    "        sc = ax.scatter(lon[q==0],lat[q==0],\n",
    "                    c='r',s=1,vmin=15,vmax=30,alpha=0.5,\n",
    "                    transform=ccrs.PlateCarree())\n",
    "\n",
    "# for lat, lon, h, sig in zip(atl06_data['lat'][0:1000], \n",
    "#                             atl06_data['lon'][0:1000], \n",
    "#                             atl06_data['h'], \n",
    "#                             atl06_data['h_sig']):\n",
    "#     sc = ax.scatter(lon[(h>thr)&(sig<sig_thr)],lat[(h>thr)&(sig<sig_thr)],\n",
    "#                     c=h[(h>thr)&(sig<sig_thr)],s=1,vmin=0,vmax=30,alpha=0.5,\n",
    "#                     transform=ccrs.PlateCarree())\n",
    "plt.colorbar(sc)\n",
    "\n",
    "# for lat, lon, h, sig in zip(atl06_data['lat'][0:1000], \n",
    "#                             atl06_data['lon'][0:1000], \n",
    "#                             atl06_data['h'], \n",
    "#                             atl06_data['h_sig']):\n",
    "#     ax.plot(lon[abs(h)<thr],lat[abs(h)<thr],'.r', transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ttend = datetime.now()\n",
    "print('Runtime was: ', ttend - ttstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import h5py\n",
    "# import dateutil.parser as dparser\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "# from pyproj import Transformer\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from netCDF4 import Dataset\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # data_directory \n",
    "    \n",
    "# # Load BedMachine ice mask.  This is unfortunately a bit slow...\n",
    "# # 0 = ocean, 1 = ice-free land, 2 = grounded ice, 3 = floating ice, 4 = lake Vostok\n",
    "# # maskfile = '/Users/lipovsky/Downloads/BedMachineAntarctica_2019-11-05_v01.nc'\n",
    "# maskfile = '/Users/bradlipovsky/Downloads/BedMachineAntarctica_2020-07-15_v02.nc'\n",
    "# fh = Dataset(maskfile, mode='r')\n",
    "# x = fh.variables['x'][:]\n",
    "# y = np.flipud(fh.variables['y'][:])\n",
    "# mask = np.flipud(fh.variables['mask'][:])\n",
    "\n",
    "# def mask_nearest (x0, y0):\n",
    "#     xi = np.abs(x-x0).argmin()\n",
    "#     yi = np.abs(y-y0).argmin()\n",
    "#     return mask[yi,xi]\n",
    "\n",
    "# transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3031\")\n",
    "\n",
    "# ttstart = datetime.now()\n",
    "\n",
    "# file_list = os.listdir(data_directory)\n",
    "# files = [f for f in file_list if f.endswith('.h5')]\n",
    "\n",
    "# print(\"Found %i files in the provided directory\"%len(files))\n",
    "\n",
    "# atl06_data = {\"lat\":list(),\"lon\":list(),\"h\":list(),\"azimuth\":list(),\n",
    "#               \"h_sig\":list(),\"rgt\":list(),\"time\":list(), #\"acquisition_number\":list(),\n",
    "#               \"x\":list(), \"y\":list(), \"beam\":list(), \"quality\":np.array(), \"x_atc\":list(), \"geoid\":list() }\n",
    "\n",
    "# nf = len(files)\n",
    "# for f in files:\n",
    "#     FILE_NAME = os.path.join(data_directory,f)\n",
    "#     fid = h5py.File(FILE_NAME, mode='r')\n",
    "\n",
    "#     for lr in (\"l\",\"r\"):\n",
    "#         for i in range(1,4):\n",
    "#             try:\n",
    "#                 h_xatc = fid['gt%i%s/land_ice_segments/ground_track/x_atc'%(i,lr)][:]\n",
    "#                 h_li = fid['gt%i%s/land_ice_segments/h_li'%(i,lr)][:]\n",
    "#                 h_lat = fid['gt%i%s/land_ice_segments/latitude'%(i,lr)][:]\n",
    "#                 h_lon = fid['gt%i%s/land_ice_segments/longitude'%(i,lr)][:]\n",
    "#                 h_li_sigma = fid['gt%i%s/land_ice_segments/h_li_sigma'%(i,lr)][:]\n",
    "#                 seg_az = fid['gt%i%s/land_ice_segments/ground_track/seg_azimuth'%(i,lr)][:]\n",
    "#                 rgt = fid['/orbit_info/rgt'][0]\n",
    "#                 quality = fid['gt%i%s/land_ice_segments/atl06_quality_summary'%(i,lr)][:]\n",
    "#                 time = dparser.parse( fid['/ancillary_data/data_start_utc'][0] ,fuzzy=True )\n",
    "#                 beam = \"%i%s\"%(i,lr)\n",
    "#                 geoid = fid['/gt%i%s/land_ice_segments/dem/geoid_h'%(i,lr)][:]\n",
    "#                 [h_x,h_y] = transformer.transform( h_lat , h_lon )\n",
    "\n",
    "#             except KeyError:\n",
    "#     #                 print(\"wtf key error\")\n",
    "#                 continue\n",
    "\n",
    "# #             This is just used for Brunt:\n",
    "# #                         Not clear why some of the data is out of the region of interest\n",
    "# #                 if any(h_lon>0):\n",
    "# #                     continue\n",
    "\n",
    "#             # Only add the point if is in the ice shelf mask\n",
    "#             this_mask = [float(mask_nearest(XX,YY)) for XX,YY in zip(h_x,h_y)]\n",
    "\n",
    "#             h_lat[ this_mask == 3 ]\n",
    "\n",
    "#             atl06_data[\"lat\"].append( np.array([h_lat[i] for i in range(len(h_li)) if this_mask[i] > 2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect doublets\n",
    "reps = []\n",
    "for r in np.unique(ho['rgt']):\n",
    "    for t in np.unique(ho['time'][ho['rgt']==r]):\n",
    "        for b in np.unique(ho['beam'][(ho['rgt']==r) & (ho['time']==t)]):\n",
    "            reps.append (  len(  ho[(ho['rgt'] == r) & (ho['time'] == t) & (ho['beam'] == b)]))\n",
    "\n",
    "fig4,ax4=plt.subplots()\n",
    "plt.hist(reps,5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate rift widths and axis angles\n",
    "assuming that I already have an initial estimate for the orientation of the rift.  This can be added into the KF later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import nearest_points\n",
    "from pyproj import Geod\n",
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(\"EPSG:3031\", \"EPSG:4326\")\n",
    "geodesic = Geod(ellps='WGS84')\n",
    "\n",
    "rift_path = pickle.load( open( 'halloween.pickle', \"rb\" ) )\n",
    "riftx,rifty = np.array(rift_path.xy)\n",
    "\n",
    "def estimate_local_rift_orientation(pt,x,y):\n",
    "    # Given point(x,y), find the two nearest points in approximate_rift_orientation\n",
    "    # and then return the slope between these two points\n",
    "    closest = np.argmin( (pt.x-x)**2 + (pt.y-y)**2 )\n",
    "    [lat1,long1] = transformer.transform( x[closest] , y[closest] )\n",
    "    [lat2,long2] = transformer.transform( x[closest+1] ,  y[closest+1] )\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(long1, lat1, long2, lat2)\n",
    "    return fwd_azimuth\n",
    "\n",
    "def estimate_local_rift_orientation_xy(pt,x,y):\n",
    "    # Given point(x,y), find the two nearest points in approximate_rift_orientation\n",
    "    # and then return the slope between these two points\n",
    "    closest = np.argmin( (pt.x-x)**2 + (pt.y-y)**2 )\n",
    "    dx = x[closest+1] - x[closest]\n",
    "    dy = y[closest+1] - y[closest]\n",
    "    fwd_azimuth = np.arctan2(dy,dx) * 180/np.pi\n",
    "#     print('%f, %f, %f'%(dx,dy,fwd_azimuth))\n",
    "    return fwd_azimuth\n",
    "\n",
    "\n",
    "# Next step:  estimate actual rift widths for all obs\n",
    "# This just involves doing a little bit of trigonometry\n",
    "azimuth = []\n",
    "azimuth_xy = []\n",
    "for index, row in ho.iterrows():\n",
    "    azimuth.append(estimate_local_rift_orientation(row['geometry'],riftx,rifty))\n",
    "    azimuth_xy.append(estimate_local_rift_orientation_xy(row['geometry'],riftx,rifty))\n",
    "actual_width = np.abs(np.sin(np.deg2rad(azimuth - ho['azimuth']))) * ho['width']\n",
    "\n",
    "# These are the angles of the rift axis\n",
    "ho['rift-angle'] = pd.Series(np.array(azimuth), index=ho.index)\n",
    "ho['rift-angle-xy'] = pd.Series(np.array(azimuth), index=ho.index)\n",
    "ho['actual-width'] = pd.Series(np.array(actual_width), index=ho.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Kalman Filter\" (not really)\n",
    "At each time-step: 1) add new observations, 2) update old observations, 3) merge observations if they are close enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses pandas in all its glory\n",
    "\n",
    "time_vector = np.sort(np.unique(ho.time))\n",
    "time_diff = np.diff(time_vector)\n",
    "list_of_state_vectors = []\n",
    "min_seperation_threshold = 0\n",
    "seperation_list = []\n",
    "state_vector = pd.DataFrame({'x':np.array([]),'y':np.array([]),'w':np.array([]),\n",
    "                             't':np.array([]),'a':np.array([])})\n",
    "\n",
    "for t,i in zip(time_vector,np.arange(len(time_vector))):\n",
    "    new_obs = ho[ho['time']==t]\n",
    "    if i>0:\n",
    "        dt = time_diff[i-1].total_seconds()/86400/365.25 # time step in years\n",
    "    else:\n",
    "        dt = np.inf\n",
    "    print('%i. Found %i new observations at time %s with dt=%s'%(i,len(new_obs),t,dt))\n",
    "    \n",
    "\n",
    "    \n",
    "    # Update the locations of any previous measurements\n",
    "    if t > min(time_vector):\n",
    "        \n",
    "        vx_new = vx_interp(state_vector['x'],state_vector['y']).diagonal()\n",
    "        vy_new = vy_interp(state_vector['x'],state_vector['y']).diagonal()\n",
    "        \n",
    "        # Calculate strain rates\n",
    "        e11 = vx_x(state_vector['x'],state_vector['y']).diagonal()\n",
    "        e12 = ( vx_y(state_vector['x'],state_vector['y']).diagonal() \n",
    "              + vy_x(state_vector['x'],state_vector['y']).diagonal() ) / 2\n",
    "        e22 = vy_y(state_vector['x'],state_vector['y']).diagonal()\n",
    "        \n",
    "        # Direction cosines of the rift\n",
    "        dx1 = np.cos(state_vector['a'] * np.pi / 180 + np.pi/2)\n",
    "        dx2 = np.sin(state_vector['a']* np.pi / 180 + np.pi/2)\n",
    "        \n",
    "        # Total stretch on the rift\n",
    "        dw = e11 * dx1 * dx1 + e12 * dx1 * dx2 + e12 * dx2 * dx1 + e22 * dx2 * dx2\n",
    "        \n",
    "        state_vector['x'] = state_vector['x'] + dt * vx_new\n",
    "        state_vector['y'] = state_vector['y'] + dt * vy_new\n",
    "        state_vector['w'] = state_vector['w']* (1 + dt * dw )\n",
    "    \n",
    "    \n",
    "    # Add new observations\n",
    "    add_this = new_obs.rename(columns={\"x-centroid\": \"x\", \"y-centroid\": \"y\", \"width\": \"w\", \n",
    "                                       \"time\": \"t\", \"rift-angle-xy\": \"a\"})\n",
    "    state_vector = state_vector.append( add_this[[\"x\", \"y\", \"w\", \"t\",\"a\"]] )\n",
    "    list_of_state_vectors.append(pd.DataFrame(state_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7,ax7 = plt.subplots(figsize=(10,10))\n",
    "color=iter(cm.viridis(np.linspace(0,1,len(list_of_state_vectors))))\n",
    "for item,t in zip(reversed(list_of_state_vectors),reversed(time_vector)):\n",
    "    c=next(color)\n",
    "    plt.scatter(item['x']/1e3,item['y']/1e3,color=c, label='%i-%i-%i'%(t.year,t.month,t.day))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()\n",
    "plt.xlim((-694,-691))\n",
    "plt.ylim((1426,1429))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig9,ax9 = plt.subplots(7, 6,sharex='all', sharey='all',figsize=(15,15))\n",
    "\n",
    "for item,t,i in zip(list_of_state_vectors,time_vector,np.arange(len(list_of_state_vectors))):\n",
    "    plt.subplot(6,7,1+i)\n",
    "    plt.scatter(item['x']/1e3,item['y']/1e3,alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig10,ax10 = plt.subplots(figsize=(15,15))\n",
    "item = list_of_state_vectors[40]\n",
    "\n",
    "color=iter(cm.viridis(np.linspace(0,1,len(time_vector))))\n",
    "for t in time_vector:\n",
    "    c=next(color)\n",
    "    plt.scatter(item[item['t']==t]['x']/1e3,\n",
    "                item[item['t']==t]['y']/1e3,\n",
    "                color=c, label='%i-%i-%i'%(t.year,t.month,t.day))\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()\n",
    "plt.axis('image')\n",
    "plt.xlim((-705,-660))\n",
    "plt.ylim((1425,1455))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How close are rift observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins, _ = plt.hist(seperation_list, bins=20)\n",
    "\n",
    "# histogram on log scale. \n",
    "# Use non-equal bin sizes, such that they look equal on log scale.\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.clf()\n",
    "fig8 = plt.subplots()\n",
    "plt.hist(seperation_list, bins=logbins)\n",
    "plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.xlabel('Nearest Neighbor Distance (cumulative over all time steps)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid()\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does rift width vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = list_of_state_vectors[-1]\n",
    "\n",
    "xmin = latest.loc[latest['x'].idxmin()]['x']\n",
    "ymin = latest.loc[latest['x'].idxmin()]['y']\n",
    "times = latest['t'].apply(lambda x: (x- latest.iloc[0]['t']).days).values\n",
    "dist = np.sqrt( (latest['x'].to_numpy()-xmin)**2 + (latest['y'].to_numpy()-ymin)**2 )\n",
    "fig12 = plt.subplots(figsize=(15,5))\n",
    "c=plt.scatter(dist/1e3,latest['w'],c=times,label=latest['t'])\n",
    "xx = np.arange(0,46,0.01)\n",
    "plt.plot(xx,800*np.sqrt(46**2 - xx**2)/45,'-r')\n",
    "cbar=plt.colorbar(c)\n",
    "cbar.set_label('Days since first observation')\n",
    "plt.xlabel('Distance along rift (km)')\n",
    "plt.ylabel('Rift width (m)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How different are expected-versus-actual rift widths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
